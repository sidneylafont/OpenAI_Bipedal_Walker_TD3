{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5adf7a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict, deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ee9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_control(n):\n",
    "    rewards = 0\n",
    "    max_reward = -float(\"inf\")\n",
    "    env = gym.make(\"BipedalWalker-v3\")\n",
    "    observation = env.reset()\n",
    "    for i in range(n):\n",
    "        while True:\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            rewards += reward\n",
    "            if reward > max_reward:\n",
    "                max_reward = reward\n",
    "            if done:\n",
    "                print(\"End Game!: Reward: \", reward)\n",
    "                observation = env.reset()\n",
    "                break\n",
    "        env.close()\n",
    "    rewards = rewards / n\n",
    "    print(\"Average Final Reward: \", rewards)\n",
    "    print(\"Max Final Reward: \", max_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9a97d2b5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.10703787943472703\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.055409644504390544\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.20295348527034363\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.09710959613323211\n",
      "End Game!: Reward:  0.11028303787112237\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.18620821034908297\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.06360525055726607\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.13226642215251921\n",
      "End Game!: Reward:  -0.07127401012554764\n",
      "End Game!: Reward:  0.0018142666618005572\n",
      "End Game!: Reward:  0.10635405158003053\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.09173737896730386\n",
      "End Game!: Reward:  -0.048039596468210224\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  0.019456048478685206\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.12344223960240919\n",
      "End Game!: Reward:  0.06218691603342455\n",
      "End Game!: Reward:  -0.1633591539859772\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.14346064790090163\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  0.08754076441128932\n",
      "End Game!: Reward:  -0.21313558787107464\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.026252978305023107\n",
      "End Game!: Reward:  -0.1733097105125598\n",
      "End Game!: Reward:  -0.05578044513861457\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.06079932475090027\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.10686286532878875\n",
      "End Game!: Reward:  0.027147734721504047\n",
      "End Game!: Reward:  -0.016343883434930845\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.07302393551667413\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  0.06722667175531385\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.11209588372707366\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.18058497456709308\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.24870959406594553\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -0.0746645744244234\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  0.10209184155861654\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "End Game!: Reward:  -100\n",
      "Average Final Reward:  -100.45027781274067\n",
      "Max Final Reward:  0.5314847147911788\n"
     ]
    }
   ],
   "source": [
    "random_control(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f013106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77259cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_episode(env):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = observation\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8432e08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-1. -1. -1. -1.], [1. 1. 1. 1.], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5b3071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_learning(env, num_episodes, generate_episode, gamma=1.0):\n",
    "    xdata, ydata = [], []\n",
    "    max_score = -float(\"inf\")\n",
    "    tmp_scores = deque(maxlen=100) \n",
    "    avg_scores = deque(maxlen=num_episodes)\n",
    "    \n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        next_state = env.reset()\n",
    "        while True:  \n",
    "            action = env.action_space.sample()\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # whenever the episode ends, add the reward to the temporary array\n",
    "            if done:\n",
    "                # stores the maximum score over the num_episodes\n",
    "                if max_score < reward:\n",
    "                    max_score = reward\n",
    "                tmp_scores.append(reward)\n",
    "                \n",
    "                break\n",
    "                \n",
    "        if (i_episode % 10 == 0):\n",
    "            # plot updates every 10 episodes     \n",
    "            xdata.append(i_episode)\n",
    "            ydata.append(np.mean(tmp_scores))\n",
    "\n",
    "        if (i_episode % 100 == 0):\n",
    "            # updates user every 100 episodes \n",
    "            avg_scores = tmp_scores\n",
    "            print(\"\\nEpisode Number: \", i_episode)\n",
    "            print(('Best Average Reward over %d Episodes: ' % 100), np.max(avg_scores))\n",
    "\n",
    "    fig= plt.figure()\n",
    "    ax=fig.add_subplot()\n",
    "    Ln, = ax.plot(xdata,ydata)\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward Over Prior 10 Episodes')\n",
    "    ax.set_xlim([0,num_episodes])\n",
    "    ax.set_ylim([-100,10])\n",
    "    fig.savefig(\"img_monte_learning.png\")\n",
    "    print(\"\\nAverage reward after \", num_episodes,\" episodes: \",np.mean(ydata))\n",
    "    print(\"\\nMax reward after \", num_episodes, \" episodes: \", max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "670b07e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_test(env, num_episodes, alpha, gamma=1.0, eps_start=1.0, eps_decay=.99999, eps_min=0.05):\n",
    "    nA = env.action_space.shape[0]\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    epsilon = eps_start\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "        Q = update_Q(env, episode, Q, alpha, gamma)\n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e713eea",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/1000.\n",
      "Episode Number:  100\n",
      "Best Average Reward over 100 Episodes:  0.10822782421112058\n",
      "Episode 200/1000.\n",
      "Episode Number:  200\n",
      "Best Average Reward over 100 Episodes:  0.15014757639169693\n",
      "Episode 300/1000.\n",
      "Episode Number:  300\n",
      "Best Average Reward over 100 Episodes:  0.09723373850186902\n",
      "Episode 338/1000."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-90ecc8006677>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"BipedalWalker-v3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmonte_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerate_random_episode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-137d4d512f91>\u001b[0m in \u001b[0;36mmonte_learning\u001b[1;34m(env, num_episodes, generate_episode, gamma)\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\rEpisode {}/{}.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_episode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\")\n",
    "monte_learning(env, 1000, generate_random_episode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
