{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5adf7a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict, deque\n",
    "import matplotlib.pyplot as plt\n",
    "env = gym.make(\"BipedalWalker-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5ee9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_control(n):\n",
    "    rewards = 0\n",
    "    max_reward = -float(\"inf\")\n",
    "    env = gym.make(\"BipedalWalker-v3\")\n",
    "    observation = env.reset()\n",
    "    for i in range(n):\n",
    "        while True:\n",
    "            env.render()\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            rewards += reward\n",
    "            if reward > max_reward:\n",
    "                max_reward = reward\n",
    "            if done:\n",
    "                print(\"End Game!: Reward: \", reward)\n",
    "                observation = env.reset()\n",
    "                break\n",
    "        env.close()\n",
    "    rewards = rewards / n\n",
    "    print(\"Average Final Reward: \", rewards)\n",
    "    print(\"Max Final Reward: \", max_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f013106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from Hifly reference to attempt to turn continuous\n",
    "# state space into a discrete state space\n",
    "bucket_size_states = (4,5,5,5,4,5,4,5,2,4,5,4,5,2)\n",
    "dim_states = len(bucket_size_states)\n",
    "\n",
    "bucket_size_action = (20,20,20,20)\n",
    "dim_action = len(bucket_size_action)\n",
    "sBounds = [(0, math.pi),\n",
    "           (-2,2),\n",
    "           (-1,1),\n",
    "           (-1,1),\n",
    "           (0,math.pi),\n",
    "           (-2,2),\n",
    "           (0, math.pi),\n",
    "           (-2,2),\n",
    "           (0,1),\n",
    "           (0, math.pi),\n",
    "           (-2, 2),\n",
    "           (0, math.pi),\n",
    "           (-2, 2),\n",
    "           (0, 1)]\n",
    "\n",
    "def state_to_bucket(state):\n",
    "    bucket_state = []\n",
    "    for i in range(len(state)):\n",
    "        bucket_index = int((state[i]-sBounds[i][0])\n",
    "                           / (sBounds[i][1]-sBounds[i][0])*bucket_size_states[i]-1)\n",
    "        bucket_state.append(bucket_index)\n",
    "    return tuple(bucket_state)\n",
    "\n",
    "def bucket_to_action(bucket_action):\n",
    "    actionBounds = (-1, 1)\n",
    "    action = []\n",
    "    for i in range(len(bucket_action)):\n",
    "        value_action = bucket_action[i] \\\n",
    "                       / (bucket_size_action[i] -1 ) * (actionBounds[1] - actionBounds[0]) - 1\n",
    "        action.append(value_action)\n",
    "    return tuple(action)\n",
    "\n",
    "def choose_action(q_table, state, eps):\n",
    "    if random.random() < eps:\n",
    "        action = ()\n",
    "        for i in range (0, dim_action):\n",
    "            action += (random.randint(0, bucket_size_action[i]-1),)\n",
    "    else:\n",
    "        action = np.unravel_index(np.argmax(q_table[state]), q_table[state].shape)\n",
    "    return action\n",
    "\n",
    "def dd():\n",
    "    return np.zeros(bucket_size_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23fdc56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, gamma=1.0):\n",
    "    xdata, ydata = [], []\n",
    "    \n",
    "    q_table = defaultdict(dd)\n",
    "    max_score = -float(\"inf\")\n",
    "    tmp_scores = deque(maxlen=100) \n",
    "    avg_scores = deque(maxlen=num_episodes)\n",
    "    \n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # sets the state according to the state_to_bucket function\n",
    "        state = state_to_bucket(env.reset()[0:dim_states])\n",
    "        eps = .99\n",
    "        total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            # update user while learning\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            # episode = generate_episode(env) \n",
    "            # ^uncomment this if you want to see the environment \n",
    "\n",
    "            # sets the value of the q_table by specifying the action, state, and reward of the env\n",
    "            action = choose_action(q_table, state, eps)\n",
    "            action_bucket = bucket_to_action(action)\n",
    "            next_state_real, reward, done, info = env.step(action_bucket)\n",
    "            next_state = state_to_bucket(next_state_real[0:dim_states])\n",
    "            # print(next_state)\n",
    "            total_reward += reward\n",
    "            # adjust learning rate if necessary\n",
    "            q_table[state, action] = update_Q(0.01, eps, q_table, state, action, reward)\n",
    "            state = next_state\n",
    "                    \n",
    "            # whenever the episode ends, add the reward to the temporary array\n",
    "            if done:\n",
    "                # stores the maximum score over the num_episodes\n",
    "                if max_score < reward:\n",
    "                    max_score = reward\n",
    "\n",
    "                tmp_scores.append(reward)\n",
    "                break\n",
    "    \n",
    "        if (i_episode % 10 == 0):\n",
    "            # plot updates every 10 episodes     \n",
    "            xdata.append(i_episode)\n",
    "            ydata.append(np.mean(tmp_scores))\n",
    "        \n",
    "        if (i_episode % 100 == 0):\n",
    "            # updates user every 100 episodes \n",
    "            avg_scores = tmp_scores\n",
    "            print(\"\\nEpisode Number: \", i_episode)\n",
    "            print(('Best Average Reward over %d Episodes: ' % 100), np.max(avg_scores))\n",
    "            \n",
    "    fig= plt.figure()\n",
    "    ax=fig.add_subplot()\n",
    "    Ln, = ax.plot(xdata,ydata)\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward Over Prior 10 Episodes')\n",
    "    ax.set_xlim([0,num_episodes])\n",
    "    ax.set_ylim([-100,10])\n",
    "    fig.savefig(\"img_q_learning_6.png\")\n",
    "    print(\"\\nAverage reward after \", num_episodes,\" episodes: \",np.mean(ydata))\n",
    "    print(\"\\nMax reward after \", num_episodes, \" episodes: \", max_score)\n",
    "    return q_table\n",
    "\n",
    "def update_Q(alpha, gamma, q_table, state, action, reward, next_state=None):\n",
    "    currentQValue = q_table[state][action] \n",
    "    Qsa_next = np.max(q_table[next_state]) \n",
    "    # uses the current Q_Value and the next state to determine the target value less the discount factor\n",
    "    targetQValue = reward + (gamma * Qsa_next)  \n",
    "    # returns the updated value as a function of the current value, target value, less the learning rate\n",
    "    updated_value = currentQValue + (alpha * (targetQValue - currentQValue)) \n",
    "    #print(new_value)\n",
    "    return updated_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a200e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/1000.\n",
      "Episode Number:  100\n",
      "Best Average Reward over 100 Episodes:  0.08718213958489267\n",
      "Episode 200/1000.\n",
      "Episode Number:  200\n",
      "Best Average Reward over 100 Episodes:  0.12364452132582666\n",
      "Episode 300/1000.\n",
      "Episode Number:  300\n",
      "Best Average Reward over 100 Episodes:  0.11248096062138535\n",
      "Episode 400/1000.\n",
      "Episode Number:  400\n",
      "Best Average Reward over 100 Episodes:  0.13936640108049844\n",
      "Episode 500/1000.\n",
      "Episode Number:  500\n",
      "Best Average Reward over 100 Episodes:  0.09978505933912175\n",
      "Episode 600/1000.\n",
      "Episode Number:  600\n",
      "Best Average Reward over 100 Episodes:  0.09148430789144417\n",
      "Episode 700/1000.\n",
      "Episode Number:  700\n",
      "Best Average Reward over 100 Episodes:  0.10479575539471575\n",
      "Episode 800/1000.\n",
      "Episode Number:  800\n",
      "Best Average Reward over 100 Episodes:  0.08121348980644529\n",
      "Episode 900/1000.\n",
      "Episode Number:  900\n",
      "Best Average Reward over 100 Episodes:  0.08315763258306605\n",
      "Episode 1000/1000.\n",
      "Episode Number:  1000\n",
      "Best Average Reward over 100 Episodes:  0.0666360466950818\n",
      "\n",
      "Average reward after  1000  episodes:  -67.78895843552104\n",
      "\n",
      "Max reward after  1000  episodes:  0.13936640108049844\n"
     ]
    }
   ],
   "source": [
    "q_results = q_learning(env, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969e377a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
